{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063bcf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 21:22:57.611765: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-14 21:22:57.614004: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-14 21:22:57.881880: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-14 21:22:59.094139: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-14 21:22:59.094538: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# IMPORTS\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence, Mapping\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks, optimizers, Sequential\n",
    "\n",
    "# ==========================================================\n",
    "# 1) INDICADORES TÉCNICOS\n",
    "# ==========================================================\n",
    "def make_tech_indicators(\n",
    "    data,\n",
    "    price_col: str = \"Close\",\n",
    "    high_col: str = \"High\",\n",
    "    low_col: str = \"Low\",\n",
    "    vol_col: str = \"Volume\",\n",
    "    safe: bool = True,          # anti-fuga: shift(1) de todos los features\n",
    "    reset_index: bool = True    # agrega columna Date en vez de dejar índice datetime\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Dada una serie temporal (pd.Series con precios o pd.DataFrame OHLCV),\n",
    "    retorna un DataFrame con columnas: Date, Price, <indicadores...>.\n",
    "    Todos los features quedan calculados solo con info hasta t\n",
    "    (y si safe=True se desplazan 1 día para predecir t+1).\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- Normalización de entrada ----------\n",
    "    if isinstance(data, pd.Series):\n",
    "        df = data.to_frame(name=price_col).copy()\n",
    "    else:\n",
    "        df = data.copy()\n",
    "\n",
    "    if df.index.name is None:\n",
    "        df.index.name = \"Date\"\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Asegura columnas esperadas y dtypes\n",
    "    for c in [price_col, high_col, low_col, vol_col]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float64\")\n",
    "    if price_col not in df.columns:\n",
    "        raise ValueError(f\"Falta columna de precio '{price_col}'\")\n",
    "\n",
    "    has_hl = (high_col in df.columns) and (low_col in df.columns)\n",
    "    has_vol = vol_col in df.columns\n",
    "\n",
    "    # Alias locales\n",
    "    close = df[price_col]\n",
    "    high  = df[high_col] if has_hl else None\n",
    "    low   = df[low_col]  if has_hl else None\n",
    "    vol   = df[vol_col].replace(0, np.nan).astype(\"float64\") if has_vol else None\n",
    "\n",
    "    # ---------- Helpers internos ----------\n",
    "    ema = lambda s, span: s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "    def true_range():\n",
    "        prev_close = close.shift(1)\n",
    "        tr1 = (high - low) if has_hl else (close - close) * np.nan\n",
    "        tr2 = (high - prev_close).abs() if has_hl else (close - prev_close).abs()\n",
    "        tr3 = (low - prev_close).abs() if has_hl else (prev_close - close).abs()\n",
    "        return pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "\n",
    "    def rsi(n=14):\n",
    "        delta = close.diff()\n",
    "        gain = delta.clip(lower=0)\n",
    "        loss = -delta.clip(upper=0)\n",
    "        avg_gain = gain.ewm(alpha=1/n, min_periods=n, adjust=False).mean()\n",
    "        avg_loss = loss.ewm(alpha=1/n, min_periods=n, adjust=False).mean()\n",
    "        rs = avg_gain / avg_loss.replace(0, np.nan)\n",
    "        return (100 - (100 / (1 + rs))).fillna(50.0)\n",
    "\n",
    "    def stochastic(n=14, d=3):\n",
    "        if not has_hl:\n",
    "            return pd.Series(np.nan, index=df.index), pd.Series(np.nan, index=df.index)\n",
    "        low_n = low.rolling(n).min()\n",
    "        high_n = high.rolling(n).max()\n",
    "        den = (high_n - low_n).replace(0, np.nan)\n",
    "        k = 100 * (close - low_n) / den\n",
    "        dline = k.rolling(d).mean()\n",
    "        return k, dline\n",
    "\n",
    "    def bollinger(n=20, k=2):\n",
    "        ma = close.rolling(n).mean()\n",
    "        sd = close.rolling(n).std()\n",
    "        upper, lower = ma + k*sd, ma - k*sd\n",
    "        bw = (upper - lower) / ma\n",
    "        den = (upper - lower).replace(0, np.nan)\n",
    "        pb = (close - lower) / den\n",
    "        return bw, pb\n",
    "\n",
    "    def adx_block(n=14):\n",
    "        if not has_hl:\n",
    "            nan = pd.Series(np.nan, index=df.index)\n",
    "            return nan, nan, nan, nan\n",
    "        up_move = high.diff()\n",
    "        down_move = -low.diff()\n",
    "        plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0.0)\n",
    "        minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0.0)\n",
    "        tr = true_range()\n",
    "        atr = tr.ewm(alpha=1/n, adjust=False).mean().replace(0, np.nan)\n",
    "        plus_di = 100 * pd.Series(plus_dm, index=df.index).ewm(alpha=1/n, adjust=False).mean() / atr\n",
    "        minus_di = 100 * pd.Series(minus_dm, index=df.index).ewm(alpha=1/n, adjust=False).mean() / atr\n",
    "        den = (plus_di + minus_di).replace(0, np.nan)\n",
    "        dx = 100 * (plus_di - minus_di).abs() / den\n",
    "        adx = dx.ewm(alpha=1/n, adjust=False).mean()\n",
    "        return atr, adx, plus_di, minus_di\n",
    "\n",
    "    def obv():\n",
    "        if not has_vol:\n",
    "            return pd.Series(np.nan, index=df.index)\n",
    "        return (np.sign(close.diff()).fillna(0) * vol.fillna(0)).cumsum()\n",
    "\n",
    "    def mfi(n=14):\n",
    "        if not (has_hl and has_vol):\n",
    "            return pd.Series(np.nan, index=df.index)\n",
    "        tp = (high + low + close) / 3.0\n",
    "        mf = tp * vol.fillna(0.0)\n",
    "        pos_flow = np.where(tp > tp.shift(1), mf, 0.0)\n",
    "        neg_flow = np.where(tp < tp.shift(1), mf, 0.0)\n",
    "        pos = pd.Series(pos_flow, index=df.index, dtype=\"float64\").rolling(n).sum()\n",
    "        neg = pd.Series(neg_flow, index=df.index, dtype=\"float64\").rolling(n).sum()\n",
    "        mfr = pos / neg.replace(0, np.nan)\n",
    "        return (100 - (100 / (1 + mfr))).fillna(50.0)\n",
    "\n",
    "    def cci(n=20):\n",
    "        if not has_hl:\n",
    "            return pd.Series(np.nan, index=df.index)\n",
    "        tp = (high + low + close) / 3.0\n",
    "        sma = tp.rolling(n).mean()\n",
    "        mad = (tp - sma).abs().rolling(n).mean()\n",
    "        return (tp - sma) / (0.015 * mad)\n",
    "\n",
    "    def williams_r(n=14):\n",
    "        if not has_hl:\n",
    "            return pd.Series(np.nan, index=df.index)\n",
    "        hh = high.rolling(n).max()\n",
    "        ll = low.rolling(n).min()\n",
    "        denom = (hh - ll).replace(0, np.nan)\n",
    "        return -100 * (hh - close) / denom\n",
    "\n",
    "    # ---------- Cálculo de indicadores ----------\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[\"Price\"] = close.astype(\"float64\")\n",
    "\n",
    "    out[\"Open\"]   = df[\"Open\"].astype(\"float64\") if \"Open\" in df.columns else np.nan\n",
    "    out[\"High\"]   = high if has_hl else np.nan\n",
    "    out[\"Low\"]    = low if has_hl else np.nan\n",
    "    out[\"Volume\"] = df[vol_col] if has_vol else np.nan\n",
    "\n",
    "    # Tendencia\n",
    "    for n in [5, 10, 20, 50, 200]:\n",
    "        out[f\"SMA_{n}\"] = close.rolling(n).mean()\n",
    "    for n in [5, 10, 20, 50]:\n",
    "        out[f\"EMA_{n}\"] = ema(close, n)\n",
    "\n",
    "    macd = ema(close, 12) - ema(close, 26)\n",
    "    macd_sig = ema(macd, 9)\n",
    "    out[\"MACD\"]        = macd\n",
    "    out[\"MACD_signal\"] = macd_sig\n",
    "    out[\"MACD_hist\"]   = macd - macd_sig\n",
    "\n",
    "    # Momentum\n",
    "    for h in [1, 5, 10]:\n",
    "        out[f\"ROC_{h}\"] = (close - close.shift(h)) / close.shift(h)\n",
    "        out[f\"MOM_{h}\"] = close - close.shift(h)\n",
    "    out[\"RSI_14\"] = rsi(14)\n",
    "\n",
    "    # Volatilidad y bandas\n",
    "    out[\"VOL_20\"] = np.log(close / close.shift(1)).rolling(20).std()\n",
    "    bw, pb = bollinger(20, 2)\n",
    "    out[\"BB_BW\"] = bw\n",
    "    out[\"BB_PB\"] = pb\n",
    "\n",
    "    # Osciladores con H/L\n",
    "    k, d = stochastic(14, 3)\n",
    "    out[\"STOCH_K\"]  = k\n",
    "    out[\"STOCH_D\"]  = d\n",
    "    out[\"WILLR_14\"] = williams_r(14)\n",
    "    out[\"CCI_20\"]   = cci(20)\n",
    "\n",
    "    # Rango y dirección\n",
    "    atr, adx, plus_di, minus_di = adx_block(14)\n",
    "    out[\"ATR_14\"]   = atr\n",
    "    out[\"ADX_14\"]   = adx\n",
    "    out[\"PLUS_DI\"]  = plus_di\n",
    "    out[\"MINUS_DI\"] = minus_di\n",
    "\n",
    "    # Flujo / volumen\n",
    "    out[\"OBV\"]     = obv()\n",
    "    out[\"MFI_14\"]  = mfi(14)\n",
    "\n",
    "    # Extras\n",
    "    out[\"BIAS_12\"] = (close - close.rolling(12).mean()) / close.rolling(12).mean()\n",
    "    updays = (close.diff() > 0).astype(int)\n",
    "    out[\"PSY_12\"] = 100 * updays.rolling(12).mean()\n",
    "    out[\"BBI\"] = (\n",
    "        close.rolling(3).mean()\n",
    "        + close.rolling(6).mean()\n",
    "        + close.rolling(12).mean()\n",
    "        + close.rolling(24).mean()\n",
    "    ) / 4.0\n",
    "\n",
    "    # CHO (Chaikin Oscillator)\n",
    "    if has_hl and has_vol:\n",
    "        clv = ((close - low) - (high - close)) / (high - low)\n",
    "        clv = clv.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        adl = (clv * vol.fillna(0)).cumsum()\n",
    "        out[\"CHO_3_10\"] = ema(adl, 3) - ema(adl, 10)\n",
    "    else:\n",
    "        out[\"CHO_3_10\"] = np.nan\n",
    "\n",
    "    # MASS / WVAD / AR-BR / CR\n",
    "    if has_hl:\n",
    "        diff_hl = (high - low).abs()\n",
    "        e1 = diff_hl.ewm(span=9, adjust=False).mean()\n",
    "        e2 = e1.ewm(span=9, adjust=False).mean()\n",
    "        ratio = e1 / e2.replace(0, np.nan)\n",
    "        out[\"MASS_25\"] = ratio.rolling(25).sum()\n",
    "    else:\n",
    "        out[\"MASS_25\"] = np.nan\n",
    "\n",
    "    if has_hl and has_vol:\n",
    "        rng = (high - low).replace(0, np.nan)\n",
    "        out[\"WVAD_24\"] = (((close - df.get(\"Open\", close)) / rng) * vol).rolling(24).sum()\n",
    "\n",
    "        ar_num = (high - df.get(\"Open\", close)).rolling(26).sum()\n",
    "        ar_den = (df.get(\"Open\", close) - low).rolling(26).sum().replace(0, np.nan)\n",
    "        out[\"AR_26\"] = 100 * ar_num / ar_den\n",
    "\n",
    "        cp = close.shift(1)\n",
    "        br_num = (high - cp).rolling(26).sum()\n",
    "        br_den = (cp - low).rolling(26).sum().replace(0, np.nan)\n",
    "        out[\"BR_26\"] = 100 * br_num / br_den\n",
    "\n",
    "        mid   = (high + low + 2*close) / 4\n",
    "        mid_y = mid.shift(1)\n",
    "        up    = (high - mid_y).clip(lower=0)\n",
    "        down  = (mid_y - low).clip(lower=0)\n",
    "        out[\"CR_26\"] = 100 * up.rolling(26).sum() / down.rolling(26).sum().replace(0, np.nan)\n",
    "    else:\n",
    "        out[\"WVAD_24\"] = np.nan\n",
    "        out[\"AR_26\"]   = np.nan\n",
    "        out[\"BR_26\"]   = np.nan\n",
    "        out[\"CR_26\"]   = np.nan\n",
    "\n",
    "    # ---------- Anti-fuga ----------\n",
    "    if safe:\n",
    "        feat_cols = [c for c in out.columns if c != \"Price\"]\n",
    "        out[feat_cols] = out[feat_cols].shift(1)\n",
    "\n",
    "    # ---------- Limpieza ----------\n",
    "    out = out.astype(\"float64\")\n",
    "    out = out.replace([np.inf, -np.inf], np.nan)\n",
    "    out = out.dropna().copy()\n",
    "\n",
    "    if reset_index:\n",
    "        out = out.rename_axis(\"Date\").reset_index()\n",
    "\n",
    "    return out\n",
    "\n",
    "# ==========================================================\n",
    "# 2) CORE TIME-SERIES: build_xy, splits, escaladores, secuencias\n",
    "# ==========================================================\n",
    "def build_xy(features_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    A partir de features técnicos devuelve:\n",
    "    - X: todas las columnas de features (incluyendo Price).\n",
    "    - y: Price_next = Price(t+1).\n",
    "    \"\"\"\n",
    "    df = features_df.copy()\n",
    "    if \"Date\" in df.columns:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "        df = df.set_index(\"Date\").sort_index()\n",
    "\n",
    "    df[\"Price_next\"] = df[\"Price\"].shift(-1)\n",
    "    df = df.dropna()\n",
    "\n",
    "    y = df[\"Price_next\"].astype(float)\n",
    "    X = df.drop(columns=[\"Price_next\"])\n",
    "\n",
    "    const_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n",
    "    if const_cols:\n",
    "        X = X.drop(columns=const_cols)\n",
    "\n",
    "    assert X.index.equals(y.index)\n",
    "    return X, y\n",
    "\n",
    "@dataclass\n",
    "class SplitDates:\n",
    "    train_end: str\n",
    "    val_end: str\n",
    "    test_end: Optional[str] = None  # si None, test = resto posterior a val_end\n",
    "\n",
    "def time_splits(X: pd.DataFrame, y: pd.Series, dates: SplitDates):\n",
    "    idx = X.index\n",
    "    train = idx <= pd.to_datetime(dates.train_end)\n",
    "    val   = (idx > pd.to_datetime(dates.train_end)) & (idx <= pd.to_datetime(dates.val_end))\n",
    "    test  = idx > pd.to_datetime(dates.val_end) if dates.test_end is None else (\n",
    "            (idx > pd.to_datetime(dates.val_end)) & (idx <= pd.to_datetime(dates.test_end)))\n",
    "    return (X.loc[train], y.loc[train]), (X.loc[val], y.loc[val]), (X.loc[test], y.loc[test])\n",
    "\n",
    "class TimeScaler:\n",
    "    \"\"\"\n",
    "    MinMax para X, fit SOLO en train.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_range=(0, 1)):\n",
    "        self.scaler = MinMaxScaler(feature_range=feature_range)\n",
    "        self.cols_: list[str] = []\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame):\n",
    "        self.scaler.fit(X_train.values)\n",
    "        self.cols_ = list(X_train.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        missing = [c for c in self.cols_ if c not in X.columns]\n",
    "        extra   = [c for c in X.columns if c not in self.cols_]\n",
    "        assert not missing, f\"Faltan columnas para transformar: {missing}\"\n",
    "        if extra:\n",
    "            print(f\"[AVISO] Columnas extra ignoradas en transform: {extra}\")\n",
    "        X2 = X[self.cols_].values\n",
    "        return self.scaler.transform(X2)\n",
    "\n",
    "class TimeScalerY:\n",
    "    \"\"\"\n",
    "    MinMax para y, fit SOLO en train.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def fit(self, y_train):\n",
    "        self.scaler.fit(y_train.reshape(-1, 1))\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        return self.scaler.transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "    def inverse(self, y_scaled):\n",
    "        return self.scaler.inverse_transform(y_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "def make_sequences(\n",
    "    X_arr: np.ndarray,\n",
    "    y_arr: np.ndarray,\n",
    "    dates_arr: np.ndarray,\n",
    "    lookback: int = 60,\n",
    "):\n",
    "    \"\"\"\n",
    "    Cada muestra: ventana [t-lookback+1 ... t] -> predice y[t].\n",
    "    \"\"\"\n",
    "    n = len(y_arr)\n",
    "    X_seq, y_seq, d_seq = [], [], []\n",
    "\n",
    "    for t in range(lookback - 1, n):\n",
    "        X_seq.append(X_arr[t - lookback + 1:t + 1, :])\n",
    "        y_seq.append(y_arr[t])\n",
    "        d_seq.append(dates_arr[t])\n",
    "\n",
    "    return (\n",
    "        np.array(X_seq, dtype=np.float32),\n",
    "        np.array(y_seq, dtype=np.float32),\n",
    "        np.array(d_seq),\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class LSTMPrepConfig:\n",
    "    lookback: int = 5\n",
    "    horizon: int = 1  # solo informativo\n",
    "    selected_features: Optional[Sequence[str]] = None\n",
    "\n",
    "def prepare_lstm_data(\n",
    "    features_df: pd.DataFrame,\n",
    "    split_dates: SplitDates,\n",
    "    cfg: LSTMPrepConfig = LSTMPrepConfig(),\n",
    "):\n",
    "    \"\"\"\n",
    "    Construye secuencias escaladas para LSTM / modelos tabulares (flattened).\n",
    "    \"\"\"\n",
    "    X, y = build_xy(features_df)\n",
    "\n",
    "    if cfg.selected_features is not None:\n",
    "        keep = [c for c in cfg.selected_features if c in X.columns]\n",
    "        X = X[keep]\n",
    "        assert list(X.columns) == keep\n",
    "\n",
    "    (X_tr, y_tr), (X_va, y_va), (X_te, y_te) = time_splits(X, y, split_dates)\n",
    "\n",
    "    scaler_x = TimeScaler().fit(X_tr)\n",
    "    X_all_scaled = scaler_x.transform(X)\n",
    "\n",
    "    y_all    = y.values.astype(np.float32)\n",
    "    y_tr_arr = y_tr.values.astype(np.float32)\n",
    "\n",
    "    scaler_y      = TimeScalerY().fit(y_tr_arr)\n",
    "    y_all_scaled  = scaler_y.transform(y_all)\n",
    "    dates_all     = X.index.to_numpy()\n",
    "\n",
    "    X_seq, y_seq, d_seq = make_sequences(\n",
    "        X_all_scaled,\n",
    "        y_all_scaled,\n",
    "        dates_all,\n",
    "        lookback=cfg.lookback,\n",
    "    )\n",
    "\n",
    "    train_end_dt = pd.to_datetime(split_dates.train_end)\n",
    "    val_end_dt   = pd.to_datetime(split_dates.val_end)\n",
    "    test_end_dt  = pd.to_datetime(split_dates.test_end) if split_dates.test_end is not None else None\n",
    "\n",
    "    train_mask = d_seq <= train_end_dt\n",
    "    val_mask   = (d_seq > train_end_dt) & (d_seq <= val_end_dt)\n",
    "    test_mask  = d_seq > val_end_dt if test_end_dt is None else (\n",
    "                 (d_seq > val_end_dt) & (d_seq <= test_end_dt))\n",
    "\n",
    "    Xtr_seq, ytr_seq = X_seq[train_mask], y_seq[train_mask]\n",
    "    Xva_seq, yva_seq = X_seq[val_mask], y_seq[val_mask]\n",
    "    Xte_seq, yte_seq = X_seq[test_mask], y_seq[test_mask]\n",
    "\n",
    "    return {\n",
    "        \"X_train\": Xtr_seq,\n",
    "        \"y_train\": ytr_seq,\n",
    "        \"X_val\":   Xva_seq,\n",
    "        \"y_val\":   yva_seq,\n",
    "        \"X_test\":  Xte_seq,\n",
    "        \"y_test\":  yte_seq,\n",
    "        \"scaler_x\":  scaler_x,\n",
    "        \"scaler_y\":  scaler_y,\n",
    "        \"feature_cols\": list(X.columns),\n",
    "        \"lookback\": cfg.lookback,\n",
    "        \"horizon\": 1,\n",
    "        \"dates_seq\": d_seq,\n",
    "    }\n",
    "\n",
    "def prepare_tabular_from_lstm_prep(prep_dict):\n",
    "    \"\"\"\n",
    "    Convierte tensores 3D (ventanas) a matrices 2D para sklearn.\n",
    "    \"\"\"\n",
    "    def _flat(Xseq):\n",
    "        n, L, f = Xseq.shape\n",
    "        return Xseq.reshape(n, L * f)\n",
    "\n",
    "    Xtr = _flat(prep_dict[\"X_train\"])\n",
    "    Xva = _flat(prep_dict[\"X_val\"])\n",
    "    Xte = _flat(prep_dict[\"X_test\"])\n",
    "    ytr = prep_dict[\"y_train\"]\n",
    "    yva = prep_dict[\"y_val\"]\n",
    "    yte = prep_dict[\"y_test\"]\n",
    "    return Xtr, ytr, Xva, yva, Xte, yte \n",
    "\n",
    "# ==========================================================\n",
    "# 3) GA PARA SELECCIÓN DE FEATURES\n",
    "# ==========================================================\n",
    "def evaluar_subset_TS(X: pd.DataFrame, y: pd.Series, mask: np.ndarray, n_splits=5) -> float:\n",
    "    \"\"\"\n",
    "    Fitness del GA: R² medio (TimeSeriesSplit + Ridge) del subset de columnas.\n",
    "    \"\"\"\n",
    "    sel_cols = X.columns[mask]\n",
    "    if len(sel_cols) < 3:\n",
    "        return -1e9\n",
    "\n",
    "    Xs = X[sel_cols].values\n",
    "    ys = y.values\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    modelo = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"reg\", Ridge(alpha=1.0))\n",
    "    ])\n",
    "\n",
    "    r2_list = []\n",
    "    for tr_idx, te_idx in tscv.split(Xs):\n",
    "        Xtr, Xte = Xs[tr_idx], Xs[te_idx]\n",
    "        ytr, yte = ys[tr_idx], ys[te_idx]\n",
    "        modelo.fit(Xtr, ytr)\n",
    "        pr = modelo.predict(Xte)\n",
    "        r2_list.append(r2_score(yte, pr))\n",
    "\n",
    "    if not r2_list:\n",
    "        return -1e9\n",
    "\n",
    "    r2_mean = float(np.mean(r2_list))\n",
    "    if not np.isfinite(r2_mean):\n",
    "        r2_mean = -1e9\n",
    "    return r2_mean\n",
    "\n",
    "def _multipoint_crossover(p1: np.ndarray, p2: np.ndarray, n_points=2):\n",
    "    L = len(p1)\n",
    "    if L < 2:\n",
    "        return p1.copy(), p2.copy()\n",
    "    pts = np.sort(np.random.choice(np.arange(1, L), size=min(n_points, max(1, L-1)), replace=False))\n",
    "    child1, child2 = p1.copy(), p2.copy()\n",
    "    swap = False\n",
    "    last = 0\n",
    "    for pt in list(pts) + [L]:\n",
    "        if swap:\n",
    "            child1[last:pt], child2[last:pt] = p2[last:pt], p1[last:pt]\n",
    "        last = pt\n",
    "        swap = not swap\n",
    "    return child1, child2\n",
    "\n",
    "def _bitflip_mutation(ind: np.ndarray, p=0.003):\n",
    "    flips = np.random.rand(ind.size) < p\n",
    "    ind[flips] = ~ind[flips]\n",
    "    return ind\n",
    "\n",
    "def _ensure_min_features(ind: np.ndarray, kmin=3):\n",
    "    on = ind.sum()\n",
    "    if on < kmin:\n",
    "        idx0 = np.where(~ind)[0]\n",
    "        if idx0.size > 0:\n",
    "            turn_on = np.random.choice(idx0, size=min(kmin-on, idx0.size), replace=False)\n",
    "            ind[turn_on] = True\n",
    "    return ind\n",
    "\n",
    "@dataclass\n",
    "class GAConfig:\n",
    "    pop_size: int = 100\n",
    "    p_crossover: float = 0.8\n",
    "    p_mutation: float = 0.06\n",
    "    generations: int = 100\n",
    "    n_points_cx: int = 2\n",
    "    elitism: int = 1\n",
    "    min_features: int = 3\n",
    "    n_splits_cv: int = 5\n",
    "    random_state: int = 42\n",
    "\n",
    "class GASelector:\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series, cfg: GAConfig):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.cfg = cfg\n",
    "        np.random.seed(cfg.random_state)\n",
    "        self.n_feat = X.shape[1]\n",
    "\n",
    "        self.pop = np.random.rand(cfg.pop_size, self.n_feat) < 0.5\n",
    "        for i in range(cfg.pop_size):\n",
    "            self.pop[i] = _ensure_min_features(self.pop[i], cfg.min_features)\n",
    "\n",
    "        self.history_best = []   # (gen, fitness, mask)\n",
    "        self.history_scores = [] # fitness por generación\n",
    "\n",
    "    def _fitness_all(self, pop):\n",
    "        fits = np.zeros(pop.shape[0], dtype=float)\n",
    "        for i, mask in enumerate(pop):\n",
    "            fits[i] = evaluar_subset_TS(self.X, self.y, mask, n_splits=self.cfg.n_splits_cv)\n",
    "        return fits\n",
    "\n",
    "    def _roulette_select(self, fits, k):\n",
    "        f = np.array(fits, dtype=float)\n",
    "        f = f - f.min() + 1e-12\n",
    "        p = f / f.sum() if f.sum() > 0 else np.ones_like(f) / len(f)\n",
    "        idx = np.random.choice(len(f), size=k, replace=True, p=p)\n",
    "        return idx\n",
    "\n",
    "    def run(self):\n",
    "        cfg = self.cfg\n",
    "        pop = self.pop.copy()\n",
    "        for gen in range(cfg.generations):\n",
    "            fits = self._fitness_all(pop)\n",
    "            self.history_scores.append(fits)\n",
    "\n",
    "            ibest = int(np.argmax(fits))\n",
    "            self.history_best.append((gen, float(fits[ibest]), pop[ibest].copy()))\n",
    "\n",
    "            elite_idx = np.argsort(-fits)[:cfg.elitism]\n",
    "            elites = pop[elite_idx].copy()\n",
    "\n",
    "            parent_idx = self._roulette_select(fits, cfg.pop_size)\n",
    "            parents = pop[parent_idx]\n",
    "\n",
    "            next_pop = []\n",
    "            for i in range(0, cfg.pop_size, 2):\n",
    "                p1 = parents[i].copy()\n",
    "                p2 = parents[i+1].copy() if i+1 < cfg.pop_size else parents[0].copy()\n",
    "                if np.random.rand() < cfg.p_crossover:\n",
    "                    c1, c2 = _multipoint_crossover(p1, p2, n_points=cfg.n_points_cx)\n",
    "                else:\n",
    "                    c1, c2 = p1, p2\n",
    "                c1 = _bitflip_mutation(c1, cfg.p_mutation)\n",
    "                c2 = _bitflip_mutation(c2, cfg.p_mutation)\n",
    "                c1 = _ensure_min_features(c1, cfg.min_features)\n",
    "                c2 = _ensure_min_features(c2, cfg.min_features)\n",
    "                next_pop.append(c1); next_pop.append(c2)\n",
    "\n",
    "            pop = np.array(next_pop[:cfg.pop_size], dtype=bool)\n",
    "\n",
    "            if cfg.elitism > 0:\n",
    "                repl_idx = np.random.choice(cfg.pop_size, size=cfg.elitism, replace=False)\n",
    "                pop[repl_idx] = elites\n",
    "\n",
    "        self.pop = pop\n",
    "        return self\n",
    "\n",
    "    def feature_importance_frequency(self):\n",
    "        masks = [m for _, _, m in self.history_best]\n",
    "        freq = np.sum(np.vstack(masks), axis=0).astype(int)\n",
    "        imp = pd.Series(freq, index=self.X.columns, name=\"freq_best_generations\").sort_values(ascending=False)\n",
    "        return imp\n",
    "\n",
    "def run_ga_feature_ranking(features_df: pd.DataFrame, cfg: GAConfig) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Ejecuta GA sobre un set de features y retorna un ranking (Series: feature -> frecuencia).\n",
    "    \"\"\"\n",
    "    X_ga, y_ga = build_xy(features_df)\n",
    "    ga = GASelector(X_ga, y_ga, cfg).run()\n",
    "    ranking = ga.feature_importance_frequency()\n",
    "    return ranking\n",
    "\n",
    "# ==========================================================\n",
    "# 4) MODELOS: LSTM, NAIVE, RF, PCA-SVR (todo en ESCALA NORMALIZADA)\n",
    "# ==========================================================\n",
    "def make_lstm(input_shape, units=128, dropout=0.2, lr=1e-3):\n",
    "    m = Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.LSTM(units, return_sequences=False),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    m.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"mse\",\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")],\n",
    "    )\n",
    "    return m\n",
    "\n",
    "def eval_lstm_once(feature_list, splits, base_features_df, lookback=5, seed=123):\n",
    "    \"\"\"\n",
    "    Entrena UN LSTM y devuelve métricas en ESCALA NORMALIZADA.\n",
    "    \"\"\"\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    prep = prepare_lstm_data(\n",
    "        features_df=base_features_df,\n",
    "        split_dates=splits,\n",
    "        cfg=LSTMPrepConfig(lookback=lookback, horizon=1, selected_features=feature_list),\n",
    "    )\n",
    "    Xtr, ytr = prep[\"X_train\"], prep[\"y_train\"]\n",
    "    Xva, yva = prep[\"X_val\"],   prep[\"y_val\"]\n",
    "    Xte, yte = prep[\"X_test\"],  prep[\"y_test\"]\n",
    "\n",
    "    model = make_lstm(Xtr.shape[1:])\n",
    "    es = callbacks.EarlyStopping(monitor=\"val_rmse\", patience=10, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        Xtr, ytr,\n",
    "        validation_data=(Xva, yva),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=[es],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    loss_tr, rmse_tr = model.evaluate(Xtr, ytr, verbose=0)\n",
    "    loss_va, rmse_va = model.evaluate(Xva, yva, verbose=0)\n",
    "    loss_te, rmse_te = model.evaluate(Xte, yte, verbose=0)\n",
    "\n",
    "    return {\n",
    "        \"train_rmse\": float(rmse_tr),\n",
    "        \"val_rmse\":   float(rmse_va),\n",
    "        \"test_rmse\":  float(rmse_te),\n",
    "        \"train_mse\":  float(loss_tr),\n",
    "        \"val_mse\":    float(loss_va),\n",
    "        \"test_mse\":   float(loss_te),\n",
    "    }\n",
    "\n",
    "def eval_lstm_multirun(feature_list, splits, base_features_df, runs=3, lookback=5, seed0=123):\n",
    "    res = [eval_lstm_once(feature_list, splits, base_features_df, lookback, seed0 + i) for i in range(runs)]\n",
    "    df = pd.DataFrame(res)\n",
    "    return {\n",
    "        \"val_mean\":       df[\"val_rmse\"].mean(),\n",
    "        \"val_std\":        df[\"val_rmse\"].std(ddof=1),\n",
    "        \"test_mean\":      df[\"test_rmse\"].mean(),\n",
    "        \"test_std\":       df[\"test_rmse\"].std(ddof=1),\n",
    "        \"val_mse_mean\":   df[\"val_mse\"].mean(),\n",
    "        \"val_mse_std\":    df[\"val_mse\"].std(ddof=1),\n",
    "        \"test_mse_mean\":  df[\"test_mse\"].mean(),\n",
    "        \"test_mse_std\":   df[\"test_mse\"].std(ddof=1),\n",
    "        \"runs\": runs,\n",
    "    }\n",
    "\n",
    "def eval_naive_persistence(features_df: pd.DataFrame, splits: SplitDates):\n",
    "    \"\"\"\n",
    "    Naive: predice Price_next[t] ≈ Price[t], usando MISMO escalado MinMax de y.\n",
    "    Devuelve MSE en ESCALA NORMALIZADA.\n",
    "    \"\"\"\n",
    "    X, y = build_xy(features_df)\n",
    "\n",
    "    price_t = X[\"Price\"].astype(float).values\n",
    "    y_all   = y.values.astype(np.float32)\n",
    "\n",
    "    idx = X.index\n",
    "    train_mask = idx <= pd.to_datetime(splits.train_end)\n",
    "    val_mask   = (idx > pd.to_datetime(splits.train_end)) & (idx <= pd.to_datetime(splits.val_end))\n",
    "    test_mask  = idx > pd.to_datetime(splits.val_end) if splits.test_end is None else (\n",
    "                 (idx > pd.to_datetime(splits.val_end)) & (idx <= pd.to_datetime(splits.test_end)))\n",
    "\n",
    "    y_train  = y_all[train_mask]\n",
    "    scaler_y = TimeScalerY().fit(y_train)\n",
    "\n",
    "    y_scaled       = scaler_y.transform(y_all)\n",
    "    price_t_scaled = scaler_y.transform(price_t)\n",
    "\n",
    "    def _mse(mask):\n",
    "        return mean_squared_error(y_scaled[mask], price_t_scaled[mask])\n",
    "\n",
    "    return {\n",
    "        \"train_mse\": _mse(train_mask),\n",
    "        \"val_mse\":   _mse(val_mask),\n",
    "        \"test_mse\":  _mse(test_mask),\n",
    "    }\n",
    "\n",
    "def eval_rf_baseline(feature_list, splits, base_features_df, lookback=5, seed=123):\n",
    "    \"\"\"\n",
    "    Random Forest sobre ventanas (flattened).\n",
    "    MSE en ESCALA NORMALIZADA de y.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    prep = prepare_lstm_data(\n",
    "        features_df=base_features_df,\n",
    "        split_dates=splits,\n",
    "        cfg=LSTMPrepConfig(lookback=lookback, horizon=1, selected_features=feature_list),\n",
    "    )\n",
    "    Xtr, ytr, Xva, yva, Xte, yte = prepare_tabular_from_lstm_prep(prep)\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        n_jobs=-1,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    rf.fit(Xtr, ytr)\n",
    "\n",
    "    pr_tr = rf.predict(Xtr)\n",
    "    pr_va = rf.predict(Xva)\n",
    "    pr_te = rf.predict(Xte)\n",
    "\n",
    "    return {\n",
    "        \"train_mse\": mean_squared_error(ytr, pr_tr),\n",
    "        \"val_mse\":   mean_squared_error(yva, pr_va),\n",
    "        \"test_mse\":  mean_squared_error(yte, pr_te),\n",
    "    }\n",
    "\n",
    "def eval_pca_svr_baseline(feature_list, splits, base_features_df, lookback=5, seed=123):\n",
    "    \"\"\"\n",
    "    PCA + SVR RBF. MSE en ESCALA NORMALIZADA de y.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    prep = prepare_lstm_data(\n",
    "        features_df=base_features_df,\n",
    "        split_dates=splits,\n",
    "        cfg=LSTMPrepConfig(lookback=lookback, horizon=1, selected_features=feature_list),\n",
    "    )\n",
    "    Xtr, ytr, Xva, yva, Xte, yte = prepare_tabular_from_lstm_prep(prep)\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"pca\", PCA(n_components=0.95, random_state=seed)),\n",
    "        (\"svr\", SVR(kernel=\"rbf\", C=10.0, epsilon=0.01)),\n",
    "    ])\n",
    "    model.fit(Xtr, ytr)\n",
    "\n",
    "    pr_tr = model.predict(Xtr)\n",
    "    pr_va = model.predict(Xva)\n",
    "    pr_te = model.predict(Xte)\n",
    "\n",
    "    return {\n",
    "        \"train_mse\": mean_squared_error(ytr, pr_tr),\n",
    "        \"val_mse\":   mean_squared_error(yva, pr_va),\n",
    "        \"test_mse\":  mean_squared_error(yte, pr_te),\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# 5) REPORTERÍA: UNA ACCIÓN Y MÚLTIPLES ACCIONES\n",
    "# ==========================================================\n",
    "def run_asset_report(\n",
    "    csv_path: str,\n",
    "    symbol: str,\n",
    "    splits: SplitDates,\n",
    "    ks: Sequence[int] = (5, 10, 20, 30),\n",
    "    lookbacks: Sequence[int] = (5,),\n",
    "    runs: int = 3,\n",
    "    ga_cfg: GAConfig = GAConfig(\n",
    "        pop_size=100,\n",
    "        p_crossover=0.8,\n",
    "        p_mutation=0.003,\n",
    "        generations=100,\n",
    "        n_points_cx=4,\n",
    "        elitism=0,\n",
    "        min_features=3,\n",
    "        n_splits_cv=5,\n",
    "        random_state=123,\n",
    "    ),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ejecuta TODO para una acción:\n",
    "      - genera features\n",
    "      - corre GA y obtiene ranking\n",
    "      - evalúa Naive, RF(todas), PCA-SVR(todas), LSTM(todas), GA-LSTM(k) para cada lookback.\n",
    "\n",
    "    Devuelve DataFrame con columnas:\n",
    "      symbol, lookback, modelo, k, val_mse, test_mse\n",
    "    \"\"\"\n",
    "    # 1) Cargar datos OHLCV y features\n",
    "    df_ohlcv = pd.read_csv(csv_path).set_index(\"Date\").sort_index()\n",
    "    features = make_tech_indicators(\n",
    "        df_ohlcv,\n",
    "        safe=False,\n",
    "        price_col=\"Close\",\n",
    "        high_col=\"High\",\n",
    "        low_col=\"Low\",\n",
    "        vol_col=\"Volume\",\n",
    "    )\n",
    "\n",
    "    # 2) GA -> ranking de features\n",
    "    ranking = run_ga_feature_ranking(features, ga_cfg)\n",
    "    feats_ordenados = list(ranking.index)\n",
    "    all_features = [c for c in features.columns if c != \"Price\"]\n",
    "\n",
    "    filas = []\n",
    "\n",
    "    # 3) Naive (no depende de lookback)\n",
    "    naive_res = eval_naive_persistence(features, splits)\n",
    "\n",
    "    for lookback in lookbacks:\n",
    "        # Baselines con todas las features\n",
    "        rf_all_res = eval_rf_baseline(all_features, splits, features, lookback=lookback)\n",
    "        pca_all_res = eval_pca_svr_baseline(all_features, splits, features, lookback=lookback)\n",
    "        lstm_all_res = eval_lstm_multirun(\n",
    "            feature_list=all_features,\n",
    "            splits=splits,\n",
    "            base_features_df=features,\n",
    "            runs=runs,\n",
    "            lookback=lookback,\n",
    "            seed0=200,\n",
    "        )\n",
    "\n",
    "        # Naive\n",
    "        filas.append({\n",
    "            \"symbol\": symbol,\n",
    "            \"lookback\": lookback,\n",
    "            \"modelo\": \"Naive\",\n",
    "            \"k\": \"-\",\n",
    "            \"val_mse\": naive_res[\"val_mse\"],\n",
    "            \"test_mse\": naive_res[\"test_mse\"],\n",
    "        })\n",
    "\n",
    "        # RF / PCA-SVR / LSTM (todas)\n",
    "        filas.append({\n",
    "            \"symbol\": symbol,\n",
    "            \"lookback\": lookback,\n",
    "            \"modelo\": \"RF (todas)\",\n",
    "            \"k\": \"-\",\n",
    "            \"val_mse\": rf_all_res[\"val_mse\"],\n",
    "            \"test_mse\": rf_all_res[\"test_mse\"],\n",
    "        })\n",
    "        filas.append({\n",
    "            \"symbol\": symbol,\n",
    "            \"lookback\": lookback,\n",
    "            \"modelo\": \"PCA-SVR (todas)\",\n",
    "            \"k\": \"-\",\n",
    "            \"val_mse\": pca_all_res[\"val_mse\"],\n",
    "            \"test_mse\": pca_all_res[\"test_mse\"],\n",
    "        })\n",
    "        filas.append({\n",
    "            \"symbol\": symbol,\n",
    "            \"lookback\": lookback,\n",
    "            \"modelo\": \"LSTM (todas)\",\n",
    "            \"k\": \"-\",\n",
    "            \"val_mse\": lstm_all_res[\"val_mse_mean\"],\n",
    "            \"test_mse\": lstm_all_res[\"test_mse_mean\"],\n",
    "        })\n",
    "\n",
    "        # GA-LSTM para distintos k\n",
    "        for k in ks:\n",
    "            selected = feats_ordenados[:k]\n",
    "            lstm_ga_res = eval_lstm_multirun(\n",
    "                feature_list=selected,\n",
    "                splits=splits,\n",
    "                base_features_df=features,\n",
    "                runs=runs,\n",
    "                lookback=lookback,\n",
    "                seed0=123,\n",
    "            )\n",
    "            filas.append({\n",
    "                \"symbol\": symbol,\n",
    "                \"lookback\": lookback,\n",
    "                \"modelo\": f\"GA-LSTM (k={k})\",\n",
    "                \"k\": k,\n",
    "                \"val_mse\": lstm_ga_res[\"val_mse_mean\"],\n",
    "                \"test_mse\": lstm_ga_res[\"test_mse_mean\"],\n",
    "            })\n",
    "\n",
    "    reporte = pd.DataFrame(filas)\n",
    "    return reporte\n",
    "\n",
    "def run_multi_asset_report(\n",
    "    symbol_to_path: Mapping[str, str],\n",
    "    splits: SplitDates,\n",
    "    ks: Sequence[int] = (5, 10, 20, 30),\n",
    "    lookbacks: Sequence[int] = (5,),\n",
    "    runs: int = 3,\n",
    "    ga_cfg: GAConfig = GAConfig(\n",
    "        pop_size=100,\n",
    "        p_crossover=0.8,\n",
    "        p_mutation=0.003,\n",
    "        generations=100,\n",
    "        n_points_cx=4,\n",
    "        elitism=0,\n",
    "        min_features=3,\n",
    "        n_splits_cv=5,\n",
    "        random_state=123,\n",
    "    ),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loop sobre varias acciones y concatena los reportes.\n",
    "    symbol_to_path: dict {ticker: ruta_csv}\n",
    "    \"\"\"\n",
    "    reports = []\n",
    "    for symbol, path in symbol_to_path.items():\n",
    "        rep = run_asset_report(\n",
    "            csv_path=path,\n",
    "            symbol=symbol,\n",
    "            splits=splits,\n",
    "            ks=ks,\n",
    "            lookbacks=lookbacks,\n",
    "            runs=runs,\n",
    "            ga_cfg=ga_cfg,\n",
    "        )\n",
    "        reports.append(rep)\n",
    "\n",
    "    return pd.concat(reports, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f9205",
   "metadata": {},
   "source": [
    "# Como usar (ejemplo rapido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a0208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 21:26:22.865793: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol  lookback          modelo  k  val_mse  test_mse\n",
      "  AAPL         5           Naive  - 0.000089  0.000223\n",
      "  AAPL         5      RF (todas)  - 0.000098  0.000535\n",
      "  AAPL         5 PCA-SVR (todas)  - 0.000679  0.015960\n",
      "  AAPL         5    LSTM (todas)  - 0.000117  0.000683\n",
      "  AAPL         5   GA-LSTM (k=5)  5 0.000108  0.000364\n",
      "  AAPL         5  GA-LSTM (k=10) 10 0.000103  0.000319\n",
      "  AAPL         5  GA-LSTM (k=20) 20 0.000100  0.000380\n",
      "  AAPL         5  GA-LSTM (k=30) 30 0.000105  0.000430\n",
      "  AAPL        10           Naive  - 0.000089  0.000223\n",
      "  AAPL        10      RF (todas)  - 0.000097  0.000561\n",
      "  AAPL        10 PCA-SVR (todas)  - 0.000581  0.009822\n",
      "  AAPL        10    LSTM (todas)  - 0.000110  0.000945\n",
      "  AAPL        10   GA-LSTM (k=5)  5 0.000101  0.000297\n",
      "  AAPL        10  GA-LSTM (k=10) 10 0.000093  0.000266\n",
      "  AAPL        10  GA-LSTM (k=20) 20 0.000104  0.000381\n",
      "  AAPL        10  GA-LSTM (k=30) 30 0.000117  0.000538\n"
     ]
    }
   ],
   "source": [
    "# Definir split temporal\n",
    "splits = SplitDates(\n",
    "    train_end=\"2001-01-01\",\n",
    "    val_end=\"2003-12-31\",\n",
    "    test_end=\"2004-12-31\",\n",
    ")\n",
    "\n",
    "# Una sola acción\n",
    "report_aapl = run_asset_report(\n",
    "    csv_path=\"Dataset/Stocks/aapl.us.txt\",\n",
    "    symbol=\"AAPL\",\n",
    "    splits=splits,\n",
    "    ks=(5,10,20,30),\n",
    "    lookbacks=(5, 10),   # por ejemplo probar lookback 5 y 10\n",
    "    runs=3,\n",
    ")\n",
    "\n",
    "print(report_aapl.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa952c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol  lookback          modelo  k  val_mse  test_mse\n",
      "  AAPL         5           Naive  - 0.000089  0.000223\n",
      "  AAPL         5    LSTM (todas)  - 0.000117  0.000683\n",
      "  AAPL         5   GA-LSTM (k=5)  5 0.000108  0.000364\n",
      "  AAPL         5  GA-LSTM (k=10) 10 0.000103  0.000319\n",
      "  AAPL         5  GA-LSTM (k=20) 20 0.000100  0.000380\n",
      "  AAPL         5  GA-LSTM (k=30) 30 0.000105  0.000430\n",
      "  AAPL         5      RF (todas)  - 0.000098  0.000535\n",
      "  AAPL         5 PCA-SVR (todas)  - 0.000679  0.015960\n",
      "  AAPL        10           Naive  - 0.000089  0.000223\n",
      "  AAPL        10    LSTM (todas)  - 0.000110  0.000945\n",
      "  AAPL        10   GA-LSTM (k=5)  5 0.000101  0.000297\n",
      "  AAPL        10  GA-LSTM (k=10) 10 0.000093  0.000266\n",
      "  AAPL        10  GA-LSTM (k=20) 20 0.000104  0.000381\n",
      "  AAPL        10  GA-LSTM (k=30) 30 0.000117  0.000538\n",
      "  AAPL        10      RF (todas)  - 0.000097  0.000561\n",
      "  AAPL        10 PCA-SVR (todas)  - 0.000581  0.009822\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = report_aapl.copy()\n",
    "\n",
    "orden_modelos = [\n",
    "    \"Naive\",\n",
    "    \"LSTM (todas)\",\n",
    "    \"GA-LSTM (k=5)\",\n",
    "    \"GA-LSTM (k=10)\",\n",
    "    \"GA-LSTM (k=20)\",\n",
    "    \"GA-LSTM (k=30)\",\n",
    "    \"RF (todas)\",\n",
    "    \"PCA-SVR (todas)\",\n",
    "]\n",
    "\n",
    "df[\"modelo\"] = pd.Categorical(df[\"modelo\"], categories=orden_modelos, ordered=True)\n",
    "\n",
    "tabla_ordenada = (\n",
    "    df.sort_values([\"lookback\", \"modelo\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0d6da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 test_mse             val_mse          \n",
      "lookback               5         10        5         10\n",
      "modelo_k                                               \n",
      "Naive            0.000223  0.000223  0.000089  0.000089\n",
      "LSTM (todas)     0.000683  0.000945  0.000117  0.000110\n",
      "GA-LSTM (k=5)    0.000364  0.000297  0.000108  0.000101\n",
      "GA-LSTM (k=10)   0.000319  0.000266  0.000103  0.000093\n",
      "GA-LSTM (k=20)   0.000380  0.000381  0.000100  0.000104\n",
      "GA-LSTM (k=30)   0.000430  0.000538  0.000105  0.000117\n",
      "RF (todas)       0.000535  0.000561  0.000098  0.000097\n",
      "PCA-SVR (todas)  0.015960  0.009822  0.000679  0.000581\n"
     ]
    }
   ],
   "source": [
    "# Usamos el nombre del modelo (incluye k) como índice\n",
    "wide = (\n",
    "    tabla_ordenada\n",
    "    .assign(modelo_k=tabla_ordenada[\"modelo\"])  # ya lleva el k en el string\n",
    "    .pivot_table(\n",
    "        index=\"modelo_k\",\n",
    "        columns=\"lookback\",\n",
    "        values=[\"val_mse\", \"test_mse\"],\n",
    "        observed=True,  # <<< agrega esto\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "\n",
    "# Ordena las columnas para que primero vaya val_mse y luego test_mse por lookback\n",
    "wide = wide.sort_index(axis=1, level=[0,1])\n",
    "\n",
    "# Opcional: redondear\n",
    "wide = wide.round(6)\n",
    "\n",
    "print(wide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b1d2f",
   "metadata": {},
   "source": [
    "# Usar mas acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e32c9bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m symbol_to_path = {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAAPL\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDataset/Stocks/aapl.us.txt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMSFT\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDataset/Stocks/msft.us.txt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSP500\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDataset/ETFs/spy.us.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# agrega más...\u001b[39;00m\n\u001b[32m      6\u001b[39m }\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m report_all = \u001b[43mrun_multi_asset_report\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msymbol_to_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msymbol_to_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlookbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mruns\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(report_all.to_string(index=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 931\u001b[39m, in \u001b[36mrun_multi_asset_report\u001b[39m\u001b[34m(symbol_to_path, splits, ks, lookbacks, runs, ga_cfg)\u001b[39m\n\u001b[32m    929\u001b[39m reports = []\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m symbol, path \u001b[38;5;129;01min\u001b[39;00m symbol_to_path.items():\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m     rep = \u001b[43mrun_asset_report\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m        \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m=\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlookbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlookbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m        \u001b[49m\u001b[43mruns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mga_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mga_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    940\u001b[39m     reports.append(rep)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.concat(reports, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 833\u001b[39m, in \u001b[36mrun_asset_report\u001b[39m\u001b[34m(csv_path, symbol, splits, ks, lookbacks, runs, ga_cfg)\u001b[39m\n\u001b[32m    830\u001b[39m filas = []\n\u001b[32m    832\u001b[39m \u001b[38;5;66;03m# 3) Naive (no depende de lookback)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m naive_res = \u001b[43meval_naive_persistence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lookback \u001b[38;5;129;01min\u001b[39;00m lookbacks:\n\u001b[32m    836\u001b[39m     \u001b[38;5;66;03m# Baselines con todas las features\u001b[39;00m\n\u001b[32m    837\u001b[39m     rf_all_res = eval_rf_baseline(all_features, splits, features, lookback=lookback)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 710\u001b[39m, in \u001b[36meval_naive_persistence\u001b[39m\u001b[34m(features_df, splits)\u001b[39m\n\u001b[32m    706\u001b[39m test_mask  = idx > pd.to_datetime(splits.val_end) \u001b[38;5;28;01mif\u001b[39;00m splits.test_end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\n\u001b[32m    707\u001b[39m              (idx > pd.to_datetime(splits.val_end)) & (idx <= pd.to_datetime(splits.test_end)))\n\u001b[32m    709\u001b[39m y_train  = y_all[train_mask]\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m scaler_y = \u001b[43mTimeScalerY\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    712\u001b[39m y_scaled       = scaler_y.transform(y_all)\n\u001b[32m    713\u001b[39m price_t_scaled = scaler_y.transform(price_t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 347\u001b[39m, in \u001b[36mTimeScalerY.fit\u001b[39m\u001b[34m(self, y_train)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_train):\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto_NN/.venv/lib64/python3.12/site-packages/sklearn/preprocessing/_data.py:454\u001b[39m, in \u001b[36mMinMaxScaler.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto_NN/.venv/lib64/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto_NN/.venv/lib64/python3.12/site-packages/sklearn/preprocessing/_data.py:494\u001b[39m, in \u001b[36mMinMaxScaler.partial_fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    491\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m    493\u001b[39m first_pass = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_array_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m device_ = device(X)\n\u001b[32m    503\u001b[39m feature_range = (\n\u001b[32m    504\u001b[39m     xp.asarray(feature_range[\u001b[32m0\u001b[39m], dtype=X.dtype, device=device_),\n\u001b[32m    505\u001b[39m     xp.asarray(feature_range[\u001b[32m1\u001b[39m], dtype=X.dtype, device=device_),\n\u001b[32m    506\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto_NN/.venv/lib64/python3.12/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto_NN/.venv/lib64/python3.12/site-packages/sklearn/utils/validation.py:1128\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1126\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1128\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1129\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1130\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1131\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1132\u001b[39m         )\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1135\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "symbol_to_path = {\n",
    "    \"AAPL\": \"Dataset/Stocks/aapl.us.txt\",\n",
    "    \"MSFT\": \"Dataset/Stocks/msft.us.txt\",\n",
    "    \"SP500\": \"Dataset/ETFs/spy.us.txt\"\n",
    "    # agrega más...\n",
    "}\n",
    "\n",
    "report_all = run_multi_asset_report(\n",
    "    symbol_to_path=symbol_to_path,\n",
    "    splits=splits,\n",
    "    ks=(5,10,20,30),\n",
    "    lookbacks=(5,),\n",
    "    runs=3,\n",
    ")\n",
    "\n",
    "print(report_all.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
